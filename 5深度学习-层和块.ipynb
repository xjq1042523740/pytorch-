{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2243, 0.3840, 0.8949, 0.6740, 0.4546, 0.2304, 0.1483, 0.8898, 0.9775,\n",
      "         0.7477, 0.4662, 0.6672, 0.6994, 0.1178, 0.3331, 0.9812, 0.5006, 0.3022,\n",
      "         0.9212, 0.2956],\n",
      "        [0.1418, 0.9334, 0.0509, 0.5866, 0.4924, 0.7301, 0.0022, 0.2459, 0.6182,\n",
      "         0.8326, 0.6727, 0.5267, 0.2939, 0.3149, 0.4983, 0.6196, 0.9784, 0.3597,\n",
      "         0.3546, 0.9738]])\n",
      "tensor([[ 0.1778,  0.0387, -0.4088,  0.1233,  0.1461, -0.1431,  0.2253,  0.0914,\n",
      "         -0.1323, -0.1203],\n",
      "        [ 0.2680,  0.1550, -0.2162,  0.1399,  0.1856, -0.1300,  0.0714,  0.1149,\n",
      "         -0.1651, -0.2014]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net=nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "\n",
    "X=torch.rand(2,20)\n",
    "print(X)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义块\n",
    "class MLP(nn.Module):   #多层感知机\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义块\n",
    "class MLP(nn.Module):   #多层感知机\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0622,  0.2228,  0.0411,  0.1117,  0.0425, -0.0948, -0.2438, -0.3385,\n",
       "          0.0884,  0.2098],\n",
       "        [ 0.1398,  0.0869,  0.3103,  0.1366, -0.0223, -0.0288, -0.2343, -0.2064,\n",
       "          0.1030,  0.2396]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for idx,module in enumerate(args):\n",
    "            self._modules[str(idx)]=module\n",
    "    \n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X=block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4336,  0.1523, -0.2063,  0.2644,  0.1875,  0.4090,  0.0187,  0.0654,\n",
       "         -0.1037,  0.2460],\n",
       "        [-0.1482,  0.1741, -0.2710,  0.1752,  0.0369,  0.2524,  0.0423,  0.1025,\n",
       "         -0.0267,  0.0479]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight=torch.rand((20,20),requires_grad=False)\n",
    "        self.linear=nn.Linear(20,20)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X=self.linear(X)\n",
    "        X=F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X=self.linear(X)\n",
    "        while X.abs().sum()>1:\n",
    "            X/=2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3497, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0531, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NetMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(20,64),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(64,32),\n",
    "                              nn.ReLU())\n",
    "        self.linear=nn.Linear(32,16)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "    \n",
    "chimera=nn.Sequential(NetMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyTorch]",
   "language": "python",
   "name": "conda-env-PyTorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
